import sys
import os
import torch
import torch.serialization
import torch.hub

# --- PyTorch 2.4+ Compatibility Patches ---
_compatible_types = []
for s in ['UInt16Storage', 'UInt32Storage', 'UInt64Storage', 'Int64Storage', 'BoolStorage', 'BFloat16Storage', 'Float8_e4m3fnStorage', 'Float8_e5m2Storage']:
    if not hasattr(torch, s):
        c = type(s, (torch.UntypedStorage,), {})
        setattr(torch, s, c)
        _compatible_types.append(c)
    if hasattr(torch, 'storage') and not hasattr(torch.storage, s):
        setattr(torch.storage, s, getattr(torch, s))
try:
    torch.serialization.add_safe_globals(_compatible_types)
except Exception: pass

_orig_torch_load = torch.serialization.load
def _patched_torch_load(*args, **kwargs):
    kwargs['weights_only'] = False
    return _orig_torch_load(*args, **kwargs)
torch.serialization.load = _patched_torch_load
torch.load = _patched_torch_load

_orig_hub_load = torch.hub.load
def _patched_hub_load(*args, **kwargs):
    kwargs['weights_only'] = False
    return _orig_hub_load(*args, **kwargs)
torch.hub.load = _patched_hub_load

_orig_url_load = torch.hub.load_state_dict_from_url
def _patched_url_load(*args, **kwargs):
    kwargs['weights_only'] = False
    return _orig_url_load(*args, **kwargs)
torch.hub.load_state_dict_from_url = _patched_url_load

for m_name, m in list(sys.modules.items()):
    if m:
        if hasattr(m, 'load') and (getattr(m, 'load') is _orig_torch_load):
            try: setattr(m, 'load', _patched_torch_load)
            except Exception: pass
        if hasattr(m, 'load_state_dict_from_url') and (getattr(m, 'load_state_dict_from_url') is _orig_url_load):
            try: setattr(m, 'load_state_dict_from_url', _patched_url_load)
            except Exception: pass
# -------------------------------------------------------------------------------------

import torch.nn as nn
import numpy as np
import cv2
import json
import matplotlib.pyplot as plt
import subprocess
import time
import argparse
import gc
import shutil
import PIL.Image
import PIL.ImageOps

# ==========================================
# ğŸŒ ãƒ‘ã‚¹è§£æ±º (ãƒãƒ¼ã‚¿ãƒ–ãƒ«æ§‹æˆ)
# ==========================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(BASE_DIR)
OUTPUT_DIR = os.path.join(BASE_DIR, "outputs")
DEBUG_DIR = os.path.join(OUTPUT_DIR, "debug_masks")
VIS_OUTPUT = os.path.join(BASE_DIR, "last_inference_vis.jpg")

REPOS_ROOT = os.path.join(PROJECT_ROOT, "repos")
SAM3D_ROOT = os.path.join(REPOS_ROOT, "sam-3d-body")
SAM3_ROOT = os.path.join(REPOS_ROOT, "sam3")
MOGE_ROOT = os.path.join(REPOS_ROOT, "MoGe")
# detectron2_repo ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãƒ‘ã‚¹ã«è¿½åŠ 
DETECTRON2_REPO = os.path.join(REPOS_ROOT, "detectron2_repo")
DETECTRON2_STUB = os.path.join(BASE_DIR, "detectron2_stub")

WEIGHTS_ROOT = os.path.join(PROJECT_ROOT, "weights", "body")
SAM3DB_CKPT = os.path.join(WEIGHTS_ROOT, "model.ckpt")
MHR_MODEL_PT = os.path.join(WEIGHTS_ROOT, "assets", "mhr_model.pt")
SAM3_CKPT = os.path.join(WEIGHTS_ROOT, "model.pt")

# å„ªå…ˆåº¦ã®é«˜ã„é †ã«ãƒ‘ã‚¹ã‚’è¿½åŠ  (å¾Œã‹ã‚‰ insert(0) ã™ã‚‹ã¨å„ªå…ˆåº¦ãŒé«˜ããªã‚‹)
# detectron2_repo/tools ã¨ sam-3d-body/tools ã®ç«¶åˆã‚’é¿ã‘ã‚‹ãŸã‚ã€sam-3d-body ã‚’å„ªå…ˆ
for p in [DETECTRON2_REPO, DETECTRON2_STUB, MOGE_ROOT, SAM3_ROOT, SAM3D_ROOT]:
    if os.path.exists(p) and p not in sys.path:
        sys.path.insert(0, p)

from sam3 import build_sam3_image_model
from sam3.model.sam3_image_processor import Sam3Processor
from sam_3d_body import load_sam_3d_body, SAM3DBodyEstimator
from sam_3d_body.visualization.skeleton_visualizer import SkeletonVisualizer
from sam_3d_body.metadata.mhr70 import pose_info

# (æ—§å¼ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ­ãƒ¼ãƒ€ãƒ¼ã¯ HumanDetector ã¸ã®ç§»è¡Œã«ä¼´ã„å‰Šé™¤)

def cleanup_outputs():
    if os.path.exists(OUTPUT_DIR): shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(DEBUG_DIR, exist_ok=True)

def clear_memory():
    gc.collect()
    if torch.cuda.is_available(): torch.cuda.empty_cache()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("image_path")
    parser.add_argument("--min_area", type=int, default=1000)
    parser.add_argument("--text_prompt", type=str, default="person")
    parser.add_argument("--conf_threshold", type=float, default=0.5)
    parser.add_argument("--sam3_only", action="store_true")
    parser.add_argument("--target_ids", type=str, default="")
    parser.add_argument("--use_moge", action="store_true")
    parser.add_argument("--clear_mem", action="store_true")
    parser.add_argument("--detector_name", type=str, default="sam3")
    parser.add_argument("--inference_type", type=str, default="full")
    parser.add_argument("--fov", type=float, default=70.0)
    parser.add_argument("--box_scale", type=float, default=1.2)
    parser.add_argument("--nms_thr", type=float, default=0.3)
    args = parser.parse_args()

    time_start = time.time()
    cleanup_outputs(); device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ç”»åƒèª­ã¿è¾¼ã¿ (EXIFå›è»¢å¯¾å¿œ & è‡ªå‹•ãƒªã‚µã‚¤ã‚º)
    try:
        pil_img = PIL.Image.open(args.image_path)
        pil_img = PIL.ImageOps.exif_transpose(pil_img)
        # OOMå›é¿: æœ€å¤§2048pxã«ç¸®å°
        if max(pil_img.size) > 2048:
            print(f"ğŸ“ Resizing image from {pil_img.size} to max 2048px...")
            pil_img.thumbnail((2048, 2048), PIL.Image.LANCZOS)
        img_bgr = cv2.cvtColor(np.array(pil_img.convert("RGB")), cv2.COLOR_RGB2BGR)
    except Exception as e:
        print(f"âŒ ERROR: Failed to load image {args.image_path}: {e}")
        sys.exit(1)

    # [Step 1] Detection
    print(f"--- [Step 1] Detection using '{args.detector_name}' (prompt: '{args.text_prompt}') ---")
    from tools.build_detector import HumanDetector
    
    try:
        # ãƒ©ãƒƒãƒ‘ãƒ¼ã¨ã—ã¦ã®è¨­è¨ˆ: 
        # åŸå‰‡çš„ã« sam-3d-body å…¬å¼ã® HumanDetector ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€
        # å…¬å¼ã‚³ãƒ¼ãƒ‰ãŒ HF (ãƒãƒƒãƒˆ) å›ºå®šã§ã‚ã‚‹ SAM3 ã«ã¤ã„ã¦ã®ã¿ã€
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ­ãƒ¼ã‚«ãƒ«ã« sam3.pt ã‚’ç½®ã„ã¦ã„ã‚‹å ´åˆã«é™ã‚Šã€èª­ã¿è¾¼ã¿ã‚’å·®ã—æ›¿ãˆã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œã‚’æ”¯æ´ã—ã¾ã™ã€‚
        
        det_path = ""
        if args.detector_name == "vitdet":
            det_path = WEIGHTS_ROOT

        raw_sam3_output = None # SAM3ã®ç”Ÿå‡ºåŠ›ã‚’ä¿æŒã—ã¦ãƒã‚¹ã‚¯ã‚’å–ã‚Šå‡ºã™
        if args.detector_name == "sam3":
            local_sam3 = os.path.join(WEIGHTS_ROOT, "sam3.pt")
            if os.path.exists(local_sam3):
                print(f"ğŸ“¦ Local SAM3 checkpoint found at {local_sam3}. Using local mode.")
                from sam3.model_builder import build_sam3_image_model
                from sam3.model.sam3_image_processor import Sam3Processor
                m_s3 = build_sam3_image_model(checkpoint_path=local_sam3, load_from_HF=False, device=device)
                detector = HumanDetector(name="sam3", device=device)
                detector.detector = m_s3
                detector.processor = Sam3Processor(m_s3, device=device)
            else:
                detector = HumanDetector(name=args.detector_name, device=device)
            
            # ç”Ÿã®å‡ºåŠ›ã‚’å–å¾—ã™ã‚‹ãŸã‚ã« processor ã‚’ç›´æ¥å©ã (sam3_run ã¨åŒæ§˜ã®å‡¦ç†)
            img_rgb = PIL.Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))
            inference_state = detector.processor.set_image(img_rgb)
            raw_sam3_output = detector.processor.set_text_prompt(state=inference_state, prompt=args.text_prompt)
            boxes = raw_sam3_output["boxes"].cpu().numpy()
            scores = raw_sam3_output["scores"].cpu().numpy()
            # ã‚¹ã‚³ã‚¢ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            keep = scores > args.conf_threshold
            boxes = boxes[keep]
            # HumanDetector.sam3_run ãŒè¡Œã†æ‹¡å¤§å‡¦ç†ã‚’å†ç¾ (å¼•æ•°ã‹ã‚‰åæ˜ )
            scale = args.box_scale
            enlarged_boxes = []
            for box in boxes:
                x1, y1, x2, y2 = box
                cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
                w, h = (x2 - x1) * scale, (y2 - y1) * scale
                enlarged_boxes.append([max(cx - w / 2, 0), max(cy - h / 2, 0), min(cx + w / 2, img_bgr.shape[1]), min(cy + h / 2, img_bgr.shape[0])])
            boxes = np.array(enlarged_boxes)
            # ãƒã‚¹ã‚¯ã‚‚ä¿æŒ
            raw_sam3_masks = raw_sam3_output["masks"][keep].cpu().numpy()
        else:
            detector = HumanDetector(name=args.detector_name, device=device, path=det_path)
            boxes = detector.run_human_detection(img_bgr, bbox_thr=args.conf_threshold, nms_thr=args.nms_thr)
            raw_sam3_masks = None
            
    except Exception as e:
        print(f"âŒ ERROR in Detection Initialization/Execution: {e}")
        boxes = np.array([[0, 0, img_bgr.shape[1], img_bgr.shape[0]]])
        raw_sam3_masks = None

    valid_masks = []
    print(f"  Total detected boxes: {len(boxes)}")

    for i, box in enumerate(boxes):
        x1, y1, x2, y2 = map(int, box)
        area = int((x2 - x1) * (y2 - y1))
        
        # ãƒã‚¹ã‚¯ç”Ÿæˆ: SAM3ã®å ´åˆã¯ç²¾å¯†ãªãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã€ãã‚Œä»¥å¤–ã¯çŸ©å½¢
        if raw_sam3_masks is not None and i < len(raw_sam3_masks):
            # ä¿å­˜å‰ã«ç¢ºå®Ÿã« 2æ¬¡å…ƒ boolean ã«æ•´å½¢
            mask = raw_sam3_masks[i]
            if mask.ndim > 2:
                mask = np.squeeze(mask)
                if mask.ndim > 2: mask = mask[0]
            mask = mask.astype(bool)
        else:
            mask_uint8 = np.zeros(img_bgr.shape[:2], dtype=np.uint8)
            cv2.rectangle(mask_uint8, (x1, y1), (x2, y2), 1, -1)
            mask = mask_uint8.astype(bool)

        if area > args.min_area:
            valid_masks.append({
                'id': int(i), 
                'segmentation': mask, 
                'area': area, 
                'score': 1.0, 
                'bbox': [float(x) for x in box] 
            })
        else:
            print(f"  Box {i} rejected: area {area} <= {args.min_area}")

    print(f"  Detected {len(valid_masks)} persons (after filtering).")
    
    for i, m in enumerate(valid_masks):
        dbg = img_bgr.copy()
        h, w = dbg.shape[:2]
        
        # 1. äººç‰©ã®å¼·èª¿ (é®®ã‚„ã‹ãªã‚ªãƒ¬ãƒ³ã‚¸ã®ç¸å–ã‚Š + éé¸æŠã‚¨ãƒªã‚¢ã®æ¸›å…‰)
        # OpenCVã®ã‚¨ãƒ©ãƒ¼ã¨IndexErrorã‚’é¿ã‘ã‚‹ãŸã‚ã€ãƒã‚¹ã‚¯ã‚’ç¢ºå®Ÿã« 2æ¬¡å…ƒãƒ»(H, W)ãƒ»uint8 ã«å¤‰æ›
        mask_raw = m['segmentation']
        if mask_raw.ndim > 2:
            # (1, H, W) -> (H, W) ç­‰ã€1ã®æ¬¡å…ƒã‚’ã™ã¹ã¦å–ã‚Šé™¤ã
            mask_raw = np.squeeze(mask_raw)
            if mask_raw.ndim > 2: # ãã‚Œã§ã‚‚å¤šæ¬¡å…ƒãªã‚‰æœ€åˆã®ãƒãƒ£ãƒ³ãƒãƒ«ã‚’é¸æŠ
                mask_raw = mask_raw[0]

        mask_u8 = (mask_raw.astype(np.uint8)) * 255
        mask_bool = mask_raw.astype(bool)

        ctrs, _ = cv2.findContours(mask_u8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(dbg, ctrs, -1, (0, 128, 255), 8) # ç¸å–ã‚Š
        
        # èƒŒæ™¯æ¸›å…‰ (ãƒã‚¹ã‚¯å¤–ã‚’ 1/6 ã«)
        dbg[~mask_bool] = (dbg[~mask_bool] // 6).astype(np.uint8)
        
        # 2. ä¸‹éƒ¨ã«ã€Œå¤–æ ã€ã¨ã—ã¦å·¨å¤§ãªIDãƒãƒ¼ã‚’ä»˜ã‘è¶³ã™ (ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°)
        bar_h = int(h * 0.18)
        if bar_h < 80: bar_h = 80
        
        # ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚’æ‹¡å¼µ (å…ƒã®ç”»åƒ + é»’å¸¯)
        canvas = np.zeros((h + bar_h, w, 3), dtype=np.uint8)
        canvas[0:h, 0:w] = dbg # ä¸Šéƒ¨ã«å…ƒã®ç”»åƒ(åŠ å·¥æ¸ˆã¿)ã‚’é…ç½®
        
        text = f"ID: {m['id']}"
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = h / 350.0 
        if font_scale < 1.8: font_scale = 1.8
        thickness = int(font_scale * 2.5)
        
        (t_w, t_h), _ = cv2.getTextSize(text, font, font_scale, thickness)
        tx = max((w - t_w) // 2, 0)
        # æ‹¡å¼µã—ãŸéƒ¨åˆ†ã®ä¸­å¤®ã«é…ç½®
        ty = h + (bar_h + t_h) // 2
        cv2.putText(canvas, text, (tx, ty), font, font_scale, (255, 255, 255), thickness, cv2.LINE_AA)
        
        cv2.imwrite(os.path.join(DEBUG_DIR, f"detected_person_{i}.jpg"), canvas)

    with open(os.path.join(OUTPUT_DIR, "detection_result.json"), "w") as f:
        json.dump([{'id': m['id'], 'area': m['area'], 'score': m['score'], 'bbox': m['bbox']} for m in valid_masks], f)
    
    # [Step 1 å®Œäº†] ãƒ¡ãƒ¢ãƒªã‚’å¾¹åº•çš„ã«è§£æ”¾
    print("--- Cleaning up Step 1 memory ---")
    if 'detector' in locals():
        if hasattr(detector, 'detector'): del detector.detector
        if hasattr(detector, 'processor'): del detector.processor
        del detector
    if 'raw_sam3_output' in locals(): del raw_sam3_output
    if 'inference_state' in locals(): del inference_state
    if 'm_s3' in locals(): del m_s3
    if 'raw_sam3_masks' in locals(): del raw_sam3_masks
    clear_memory()

    if args.sam3_only:
        print(f"âœ… SUCCESS. Detection complete in {time.time()-time_start:.2f}s.")
        sys.exit(0)

    # [Step 2] MoGe2: Depth
    depth_map = np.zeros(img_bgr.shape[:2], dtype=np.float32)
    if args.use_moge:
        print(f"--- [Step 2] MoGe2: Depth Estimation ---")
        import moge.model
        m_m = moge.model.import_model_class_by_version("v2").from_pretrained("Ruicheng/moge-2-vitl-normal").to(device).eval()
        with torch.no_grad():
            img_rgb_t = torch.from_numpy(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)).permute(2,0,1).float().to(device)/255.0
            inf_out = m_m.infer(img_rgb_t)
            depth_map = inf_out['depth'].cpu().numpy()
            depth_map = np.nan_to_num(depth_map, nan=0.0)
            
            # æ·±åº¦ãƒãƒƒãƒ—ã‚’é®®æ˜ã«å¯è¦–åŒ– (0ã‚’é™¤ã„ãŸæœ‰åŠ¹ç¯„å›²ã§æ­£è¦åŒ–)
            valid_mask = (depth_map > 1e-3)
            if valid_mask.any():
                v_min = np.percentile(depth_map[valid_mask], 2)
                v_max = np.percentile(depth_map[valid_mask], 98)
                d_vis = np.clip((depth_map - v_min) / (v_max - v_min + 1e-8), 0, 1)
                # æœ‰åŠ¹é ˜åŸŸä»¥å¤–ï¼ˆèƒŒæ™¯ï¼‰ã¯0ã«ã™ã‚‹
                d_vis[~valid_mask] = 0
            else:
                d_vis = depth_map
            
            d_vis = (d_vis * 255).astype(np.uint8)
            # å¥¥è¡Œãã‚’ç›´æ„Ÿçš„ã«ã™ã‚‹ãŸã‚ã«è‰²å½©ã‚’èª¿æ•´
            cv2.imwrite(os.path.join(OUTPUT_DIR, "output_depth.jpg"), cv2.applyColorMap(d_vis, cv2.COLORMAP_JET))
            
        del m_m, img_rgb_t; clear_memory()

    # [Step 3] SAM 3DB: Estimation
    print(f"--- [Step 3] SAM 3DB: 3D Recovery (Mode: {args.inference_type}) ---")
    clear_memory()
    
    to_p = [m for m in valid_masks if str(m['id']) in args.target_ids.split(",")] if args.target_ids else valid_masks
    print(f"--- Targets to process: {[m['id'] for m in to_p]} (Total: {len(to_p)}) ---")
    
    if not to_p:
        print("âš  No persons selected for 3D recovery.")
        sys.exit(0)

    model_3d, cfg_3d = load_sam_3d_body(SAM3DB_CKPT, device, MHR_MODEL_PT)
    est = SAM3DBodyEstimator(model_3d, cfg_3d)
    viz = SkeletonVisualizer(radius=4, line_width=2); viz.set_pose_meta(pose_info); v_img = img_bgr.copy()

    all_json_paths = []
    for m in to_p:
        pid = m['id']; mask = m['segmentation']; tmp = os.path.join(OUTPUT_DIR, f"temp_{pid}.jpg")
        print(f"  -> Processing target ID {pid}...")
        
        # ãƒã‚¹ã‚¯ã®å½¢çŠ¶è£œæ­£ãƒ­ã‚¸ãƒƒã‚¯ (ç•¥)
        if mask.shape[:2] != img_bgr.shape[:2]:
            if mask.ndim > 2: mask = np.squeeze(mask)
            if mask.ndim != 2 or mask.shape != img_bgr.shape[:2]:
                mask = cv2.resize(mask.astype(np.uint8), (img_bgr.shape[1], img_bgr.shape[0]), interpolation=cv2.INTER_NEAREST).astype(bool)

        crp = img_bgr.copy()
        try:
            crp[~mask] = 0 
            cv2.imwrite(tmp, crp); del crp
        except Exception as e:
            print(f" âš  Warning: Mask shape mismatch for ID {pid}: {e}.")
            x1, y1, x2, y2 = map(int, m['bbox'])
            crp = crp[y1:y2, x1:x2]; cv2.imwrite(tmp, crp); del crp

        try:
            r = est.process_one_image(tmp, inference_type=args.inference_type)
            if not r:
                print(f"    âš  Warning: No prediction returned for ID {pid}")
                continue
            r = r[0] if isinstance(r, list) else r
            print(f"    âœ… Prediction success for ID {pid}")
            for k in r:
                if torch.is_tensor(r[k]): r[k] = r[k].cpu().numpy()
                if isinstance(r[k], np.ndarray): r[k] = np.nan_to_num(r[k], nan=0.0)
            
            if args.use_moge:
                # 3Dç©ºé–“ä¸Šã®ä½ç½®ã‚’è¨ˆç®— (ãƒ”ãƒ³ãƒ›ãƒ¼ãƒ«ã‚«ãƒ¡ãƒ©ãƒ¢ãƒ‡ãƒ«)
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®FOVã‚’ä½¿ç”¨
                fov_deg = args.fov
                h_img, w_img = img_bgr.shape[:2]
                f_pix = (max(w_img, h_img) / 2) / np.tan(np.deg2rad(fov_deg) / 2)
                cx_img, cy_img = w_img / 2, h_img / 2
                
                # äººç‰©ã®2Dä¸­å¿ƒ (bboxã‚’ä½¿ç”¨)
                x1, y1, x2, y2 = m['bbox']
                px, py = (x1 + x2) / 2, (y1 + y2) / 2
                
                # æ·±åº¦ (Z) ã®å–å¾—
                yc, _ = np.where(mask)
                if len(yc) > 0:
                    z_val = float(np.nanmedian(depth_map[mask & (np.arange(img_bgr.shape[0])[:, None] >= np.percentile(yc, 90))]))
                    # X, Y ã‚ªãƒ•ã‚»ãƒƒãƒˆã®è¨ˆç®—
                    xoff = (px - cx_img) * z_val / f_pix
                    yoff = (py - cy_img) * z_val / f_pix
                    
                    # 3Dåº§æ¨™ã®æ›´æ–° (MHRå½¢å¼: X, Y, Z)
                    for k in ['pred_keypoints_3d', 'pred_vertices']:
                        r[k][..., 0] += xoff
                        r[k][..., 1] += yoff
                        r[k][..., 2] += z_val
            
            v_img = viz.draw_skeleton(v_img, np.hstack([r["pred_keypoints_2d"], np.ones((70,1))]))
            r['faces'] = est.faces; np.save(os.path.join(OUTPUT_DIR, f"output_{pid}.npy"), r)
            
            # [Step 4] Blender: FBX/BVH/OBJ Generation
            fbp = os.path.join(OUTPUT_DIR, f"output_{pid}.fbx"); tjp = os.path.join(OUTPUT_DIR, f"tjson_{pid}.json")
            all_json_paths.append(tjp)
            with open(tjp, 'w') as f: json.dump({"vertices": r["pred_vertices"].tolist(), "faces": est.faces.tolist(), "joints_mhr70": r["pred_keypoints_3d"].tolist()}, f)
            
            # FBX & BVH
            subprocess.run(["blender", "--background", "--python", os.path.join(BASE_DIR, "convert", "lib", "blender_humanoid_builder_v2.py"), "--", tjp, fbp], capture_output=True)
            if os.path.exists(fbp): 
                subprocess.run(["blender", "--background", "--python", os.path.join(BASE_DIR, "convert", "lib", "blender_bvh_crysta.py"), "--", fbp, os.path.join(OUTPUT_DIR, f"output_{pid}.bvh")], capture_output=True)
            
            # OBJ (Static Mesh) - ä»¥å‰ã® NameError: v ã‚’ä¿®æ­£
            # f-string å†…ã§ backslash ã‚’ä½¿ãˆãªã„åˆ¶ç´„ã‚’å›é¿ã™ã‚‹ãŸã‚äº‹å‰ã«ãƒ‘ã‚¹ã‚’åŠ å·¥
            obj_out = os.path.join(OUTPUT_DIR, f'output_{pid}.obj')
            obj_out_fixed = obj_out.replace('\\', '/')
            tjp_fixed = tjp.replace('\\', '/')
            blender_script_obj = f"import bpy,os,json; bpy.ops.wm.read_factory_settings(use_empty=True); d=json.load(open('{tjp_fixed}')); m=bpy.data.meshes.new('Mesh'); o=bpy.data.objects.new('Mesh',m); bpy.context.collection.objects.link(o); m.from_pydata([(v[0],v[2],-v[1]) for v in d['vertices']],[],d['faces']); bpy.context.view_layer.objects.active=o; o.select_set(True); bpy.ops.wm.obj_export(filepath='{obj_out_fixed}', export_selected_objects=True)"
            subprocess.run(["blender", "--background", "--python-expr", blender_script_obj], capture_output=True)

        except Exception as e: print(f" Error {pid}: {e}")
        finally:
            if os.path.exists(tmp): os.remove(tmp)
            if args.clear_mem: clear_memory()

    # [Step 5] Combined GLB for Preview
    if all_json_paths:
        print("--- [Step 5] Generating combined GLB for preview ---")
        comb_script = os.path.join(BASE_DIR, "convert", "lib", "blender_scene_combiner.py")
        glb_out = os.path.join(OUTPUT_DIR, "output_preview_combined.glb")
        subprocess.run(["blender", "--background", "--python", comb_script, "--", glb_out] + all_json_paths, capture_output=True)
        # ä¸è¦ãªä¸€æ™‚JSONã‚’å‰Šé™¤
        for p in all_json_paths: 
            if os.path.exists(p): os.remove(p)

    cv2.imwrite(VIS_OUTPUT, v_img)
    cv2.imwrite(os.path.join(OUTPUT_DIR, "output_vis_skeleton.jpg"), v_img)
    print(f"âœ… SUCCESS. Total time: {time.time()-time_start:.2f}s")
